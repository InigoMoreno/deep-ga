{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLHgN9P34mQ6"
      },
      "source": [
        "try:\n",
        "    # mount google drive\n",
        "    from google.colab import drive  # nopep8\n",
        "    drive_path = \"/content/drive\"\n",
        "    drive.mount(drive_path)\n",
        "    drive_folder = drive_path + \"/MyDrive/dtm/\"\n",
        "    using_colab = True\n",
        "except ModuleNotFoundError:\n",
        "    # Assume we are not on google colab,\n",
        "    drive_folder = \"data/\"\n",
        "    using_colab = False\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D54NT350wP4A"
      },
      "source": [
        "# Params and Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aCFhUMavUPK"
      },
      "source": [
        "!pip -qq install --upgrade pip\n",
        "!pip -qq install plyfile deepdiff talos GitPython numba\n",
        "!pip -qq install --upgrade imgaug\n",
        "import talos\n",
        "from deepdiff import DeepDiff\n",
        "import re\n",
        "import numpy as np\n",
        "from progressbar import progressbar, ProgressBar\n",
        "import sys\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import json\n",
        "import importlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import git\n",
        "import keras\n",
        "import keras.backend as K\n",
        "import datetime\n",
        "import imgaug.augmenters as iaa\n",
        "cuda_paths=!ls /usr/local | grep cuda-\n",
        "os.environ['CUDA_HOME'] = os.path.join(\"/usr/local/\",cuda_paths[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY389lr9moKc"
      },
      "source": [
        "if using_colab:\n",
        "    !git -C /content/deep_ga/ pull || git clone https://github.com/InigoMoreno/deep_ga\n",
        "    sys.path.append('/content/deep_ga')\n",
        "    import deep_ga  # nopep8\n",
        "    deep_ga=importlib.reload(deep_ga)\n",
        "else:\n",
        "    MODULE_PATH = \"deep_ga/deep_ga/__init__.py\"\n",
        "    MODULE_NAME = \"deep_ga\"\n",
        "    git.cmd.Git(MODULE_NAME).pull()\n",
        "    spec = importlib.util.spec_from_file_location(MODULE_NAME, MODULE_PATH)\n",
        "    deep_ga = importlib.util.module_from_spec(spec)\n",
        "    sys.modules[spec.name] = deep_ga\n",
        "    spec.loader.exec_module(deep_ga)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hRjWHFov6-U"
      },
      "source": [
        "p={\n",
        "  \"resolution\" : 1,                      # resolution of map [meters per pixel]\n",
        "  \"mapLength\"  : 80,                    # size of one side of the map [meters]\n",
        "  \"minSlopeThreshold\"  : 0.5,            # minimum slope to be counted [proportion]\n",
        "  \"maxNanPercentage\"   : 5/100,           # maximum percentage of NaNs in a patch [%]\n",
        "  \"minSlopePercentage\" : 10/100,          # minimum percentage of slope in a patch [%]\n",
        "  \"maxSlopePercentage\" : 40/100,          # maximum percentage of slope in a patch [%]\n",
        "  \"stdPatchShift\"          : 15,          # standard deviation of shift between to patches [m]\n",
        "   \"raycastHeight\": 2,\n",
        "   \"booleanDist\": False\n",
        "}\n",
        "p[\"mapLengthPixels\"]=math.ceil(p[\"mapLength\"]/p[\"resolution\"])\n",
        "deep_ga.set_scale(p[\"stdPatchShift\"])\n",
        "p[\"augment_a\"] = iaa.Sequential([\n",
        "    iaa.PerspectiveTransform(scale=0.01),\n",
        "    iaa.BlendAlphaSimplexNoise(iaa.Add(0.2), upscale_method=\"cubic\", size_px_max=4),\n",
        "])\n",
        "\n",
        "hyperparams = {\n",
        "    # \"input\": [\"raw\", \"fixsobel\", \"PConv\", \"SymConv\", \"Conv\"],\n",
        "    \"input\": [\"SymConv\"],\n",
        "    # \"batch_size\": [32, 64, 128, 256],\n",
        "    \"batch_size\": [32],\n",
        "    \"mobileNet_alpha\": [0.35, 0.5, 1.0],\n",
        "    # \"mobileNet_alpha\": [1.0],\n",
        "    # \"mobileNet_pooling\": [\"avg\", \"max\", None],\n",
        "    \"mobileNet_pooling\": [\"avg\"],\n",
        "    # \"mobileNet_weights\": [\"imagenet\"],\n",
        "    \"mobileNet_weights\": [None],\n",
        "    # \"firstLayerSize\": [4096, 1280, 1000],\"\n",
        "    \"firstLayerSize\": [0],\n",
        "    # \"dropout\": [0, 0.2, 0.5],\n",
        "    \"dropout\": [0],\n",
        "    # \"secondLayerSize\": [1000, 500, 100],\n",
        "    \"secondLayerSize\": [500,100,15,0],\n",
        "    # \"activation\": [\"relu\", None],\n",
        "    \"activation\": [\"relu\"],\n",
        "    # \"sharedWeights\": [True, False],\n",
        "    \"sharedWeights\": [True],\n",
        "    # \"loss\": [\"MSE\", \"MAE\", \"DOOMSE\", \"PCL\", \"BCE\"],\n",
        "    \"loss\": [\"DOOMSE\"],\n",
        "    \"optimizer\": [\"Adam\"],\n",
        "    # \"learning_rate\": [0.01, 0.001, 0.0001],\n",
        "    \"learning_rate\": [0.0001],\n",
        "    # \"learnEnding\": [True]\n",
        "    \"learnEnding\": [False]\n",
        "}\n",
        "\n",
        "keys= [k for k,v in hyperparams.items() if len(v)>1]\n",
        "experiment_path = os.path.relpath(os.path.join(drive_folder,\"talos_oxia_\"+\"__\".join(keys)))\n",
        "\n",
        "first_hyperparams={k:v[0] for k,v in hyperparams.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ62A1DMb3J0"
      },
      "source": [
        "# Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD2ppfJrbsjP"
      },
      "source": [
        "!pip -q install gdal wget\n",
        "import gdal, wget, os\n",
        "\n",
        "!pip -q install gdal wget\n",
        "import gdal, wget, os\n",
        "\n",
        "filename=\"oxia_planum.IMG\"\n",
        "\n",
        "filename=os.path.join(drive_folder, \"oxia_planum.IMG\")\n",
        "\n",
        "#get file if it does not exist\n",
        "#from https://www.uahirise.org/dtm/dtm.php?ID=ESP_037070_1985\n",
        "url = \"https://www.uahirise.org/PDS/DTM/ESP/ORB_037000_037099/ESP_037070_1985_ESP_037136_1985/DTEEC_037070_1985_037136_1985_L01.IMG\"\n",
        "if not os.path.isfile(filename):\n",
        "  wget.download(url,out = filename)\n",
        "\n",
        "#transform to numpy array\n",
        "gdata=gdal.Open(filename)\n",
        "height=gdata.GetRasterBand(1).ReadAsArray()\n",
        "height[height<-1e38]=np.nan\n",
        "height = cv2.resize(height, None, interpolation = cv2.INTER_AREA,\n",
        "                    fx=gdata.GetGeoTransform()[1] / p[\"resolution\"],\n",
        "                    fy=-gdata.GetGeoTransform()[5] / p[\"resolution\"])\n",
        "\n",
        "\n",
        "filename=os.path.join(drive_folder, \"jezero.IMG\")\n",
        "\n",
        "#get file if it does not exist\n",
        "#from https://www.uahirise.org/dtm/dtm.php?ID=ESP_023247_1985\n",
        "url = \"https://www.uahirise.org/PDS/DTM/ESP/ORB_023200_023299/ESP_023247_1985_ESP_022957_1985/DTEEC_023247_1985_022957_1985_U01.IMG\"\n",
        "if not os.path.isfile(filename):\n",
        "  wget.download(url,out = filename)\n",
        "\n",
        "#transform to numpy array\n",
        "gdata=gdal.Open(filename)\n",
        "height2=gdata.GetRasterBand(1).ReadAsArray()\n",
        "height2[height2<-1e38]=np.nan\n",
        "height2 = cv2.resize(height2, None, interpolation = cv2.INTER_AREA,\n",
        "                    fx=gdata.GetGeoTransform()[1] / p[\"resolution\"],\n",
        "                    fy=-gdata.GetGeoTransform()[5] / p[\"resolution\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c3N2ZunS4Kb"
      },
      "source": [
        "plt.clf()\n",
        "height-=np.nanmin(height)\n",
        "plt.imshow(height,cmap=\"viridis\")\n",
        "max=np.nanmax(height)\n",
        "plt.xlabel(\"x [m]\")\n",
        "plt.ylabel(\"y [m]\")\n",
        "cb=plt.colorbar(ticks=[0,max])\n",
        "cb.set_label(\"elevation [m]\",labelpad=-10, rotation=270)\n",
        "# plt.savefig(\"oxia.pdf\", dpi=500, bbox_inches = 'tight',\n",
        "#     pad_inches = 0)\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MItqNqL6nU35"
      },
      "source": [
        "\n",
        "def get_batch(batch_size, dem, p, seed=None):\n",
        "    \"\"\"Get a batch of patches for training\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): Size of the batch\n",
        "        dem (numpy array): elevation map\n",
        "        p (dict): parameters\n",
        "        seed (int, optional): Seed to use for randomness. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        (tuple): tuple containing\n",
        "            patches_a (numpy array): left patches of the batch\n",
        "            patches_b (numpy array): right patches of the batch\n",
        "            distances (double): distances between left and right patches\n",
        "    \"\"\"\n",
        "    # initialize random\n",
        "    random = np.random.RandomState(seed)\n",
        "\n",
        "    # initialize data\n",
        "    patches_a = np.empty(\n",
        "        (batch_size, p[\"mapLengthPixels\"], p[\"mapLengthPixels\"]))\n",
        "    patches_b = np.empty(\n",
        "        (batch_size, p[\"mapLengthPixels\"], p[\"mapLengthPixels\"]))\n",
        "    distances = np.empty((batch_size,))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # find first patch\n",
        "        patch_a = None\n",
        "        while patch_a is None:\n",
        "            xa = random.uniform(0, dem.shape[0])\n",
        "            ya = random.uniform(0, dem.shape[1])\n",
        "            patch_a = deep_ga.get_patch(dem, xa, ya, p)\n",
        "        if \"augment_a\" in p.keys() and p[\"augment_a\"] is not None:\n",
        "            patch_a = p[\"augment_a\"].augment_image(patch_a)\n",
        "        if \"raycastHeight\" in p.keys() and p[\"raycastHeight\"] is not None:\n",
        "            patch_a = deep_ga.raycast_occlusion(patch_a, p[\"raycastHeight\"])\n",
        "        patches_a[i, :, :] = patch_a - np.nanmean(patch_a)\n",
        "\n",
        "        # find second patch close to first\n",
        "        patch_b = None\n",
        "        while patch_b is None:\n",
        "            if p[\"booleanDist\"]:\n",
        "                if random.choice([True, False]):\n",
        "                    dx, dy = 0, 0\n",
        "                else:\n",
        "                    angle = random.uniform(0, 2 * math.pi)\n",
        "                    dx = p[\"stdPatchShift\"] * math.cos(angle)\n",
        "                    dy = p[\"stdPatchShift\"] * math.sin(angle)\n",
        "            else:\n",
        "                dx, dy = random.normal(\n",
        "                    scale=p[\"stdPatchShift\"] / p[\"resolution\"], size=2)\n",
        "            patch_b = deep_ga.get_patch(dem, xa + dx, ya + dy, p)\n",
        "        if \"augment_b\" in p.keys() and p[\"augment_b\"] is not None:\n",
        "            patch_b = p[\"augment_b\"].augment_image(patch_b)\n",
        "        patches_b[i, :, :] = patch_b - np.nanmean(patch_b)\n",
        "\n",
        "        # compute output function\n",
        "        distances[i] = np.linalg.norm([dx, dy]) * p[\"resolution\"]\n",
        "\n",
        "    return (patches_a, patches_b, distances)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-EDemI2bxwd"
      },
      "source": [
        "# p[\"stdPatchShift\"]=0\n",
        "# seed=140\n",
        "# seed=160\n",
        "# seed=580\n",
        "seed=np.random.randint(1000)\n",
        "print(seed)\n",
        "patches_a, patches_b,distances = get_batch(1, height, p, seed=seed)\n",
        "\n",
        "\n",
        "for i in range(1):\n",
        "  print(i)\n",
        "  W=p[\"mapLength\"]/2\n",
        "  # print(distances[i])\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.title(\"Original DTM patch\")\n",
        "  \n",
        "  plt.xlabel(\"x [m]\")\n",
        "  plt.ylabel(\"y [m]\")\n",
        "  plt.imshow(patches_b[i,:,:],extent=(-W,W,-W,W))\n",
        "\n",
        "\n",
        "  W=p[\"mapLength\"]/2\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.title(\"Artificial occlusion result\")\n",
        "  plt.xlabel(\"x [m]\")\n",
        "  # plt.ylabel(\"y [m]\")\n",
        "  plt.imshow(patches_a[i,:,:],extent=(-W,W,-W,W))\n",
        "  # plt.savefig(f\"oxia_occlusion.pdf\",  dpi=500, bbox_inches = 'tight',\n",
        "  #   pad_inches = 0)\n",
        "  plt.show()\n",
        "# plt.hist(distances)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6d2siPL3b4X"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhilK-GC3hZh"
      },
      "source": [
        "if using_colab: \n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir {os.path.join(experiment_path,\"logs\")}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsQZrrJMt7gW"
      },
      "source": [
        "import datetime\n",
        "\n",
        "def talos_model(x_train, y_train, x_val, y_val, params):\n",
        "  global experiment_path, keys, model\n",
        "\n",
        "  values=[str(params[k]) for k in keys]\n",
        "  i=\"__\".join(values)\n",
        "  i=i.replace(\".\",\"_\")\n",
        "  print(i)\n",
        "\n",
        "  # get data generator\n",
        "  tgenerator=deep_ga.PatchDataGenerator(10000,height,batch_size=params[\"batch_size\"],params=p)\n",
        "\n",
        "  # get validation data generator\n",
        "  vp=p.copy()\n",
        "  vp[\"augment_a\"]=None\n",
        "  vgenerator=deep_ga.PatchDataGenerator(10000,height2,batch_size=params[\"batch_size\"],params=vp)\n",
        "\n",
        "  with open(os.path.join(experiment_path, f\"{i}_params.json\"), 'w') as fp:\n",
        "      json.dump(params, fp)\n",
        "\n",
        "  callbacks=[]\n",
        "\n",
        "  # stop early if loss does not improve\n",
        "  callbacks.append(keras.callbacks.EarlyStopping(\n",
        "      monitor='val_loss', patience=25, verbose=1, restore_best_weights=True,min_delta=0.1))\n",
        "  \n",
        "  # assert that weights are finite\n",
        "  callbacks.append(deep_ga.AssertWeightsFinite())\n",
        "\n",
        "  # plot prediction\n",
        "  callbacks.append(deep_ga.PlotPrediction(\n",
        "      folder=os.path.join(experiment_path, f\"{i}_plots\"),\n",
        "      tgen=tgenerator,\n",
        "      vgen=vgenerator,\n",
        "      save=not using_colab\n",
        "    ))\n",
        "  \n",
        "    \n",
        "  # tensorboard callback\n",
        "  logdir = os.path.join(experiment_path,\"logs\", f\"{i}\")\n",
        "  callbacks.append(tf.keras.callbacks.TensorBoard(logdir))\n",
        "\n",
        "  # get model\n",
        "  input_a = keras.Input(shape=(\n",
        "      p[\"mapLengthPixels\"], p[\"mapLengthPixels\"]), name=\"patch_a\")\n",
        "  input_b = keras.Input(\n",
        "      shape=(p[\"mapLengthPixels\"], p[\"mapLengthPixels\"]), name=\"patch_b\")\n",
        "\n",
        "  model = deep_ga.get_model(params,input_a,input_b)\n",
        "  model = deep_ga.compile_model(model, y_val, params)\n",
        "\n",
        "  out = model.fit(tgenerator, \n",
        "                  validation_data=vgenerator,\n",
        "                  epochs=100, \n",
        "                  workers=6,\n",
        "                  callbacks=callbacks,\n",
        "                  verbose=(1 if using_colab else 2)\n",
        "                  )\n",
        "                  \n",
        "  x,y=tgenerator.get_batch(320)\n",
        "  yp=model.predict(x)[:,0]\n",
        "  print(f\"self mse loss = {keras.losses.MSE(y,yp)}\")\n",
        "\n",
        "  with open(os.path.join(experiment_path, f\"{i}_structure.json\"), 'w') as fp:\n",
        "      fp.write(model.to_json())\n",
        "  model.save_weights(os.path.join(experiment_path, f\"{i}_weights.h5\"))\n",
        "  model.save(os.path.join(experiment_path, f\"{i}.h5\"))\n",
        "  model.save(os.path.join(experiment_path, f\"{i}\"),include_optimizer=False)\n",
        "\n",
        "  return out, model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpY5qzGKk45T"
      },
      "source": [
        "\n",
        "fake_generator=deep_ga.PatchDataGenerator(10,height,batch_size=320,params=p)\n",
        "(fake_a, fake_b), fake_out = fake_generator[0]\n",
        "\n",
        "i=0\n",
        "\n",
        "\n",
        "import shutil\n",
        "if os.path.exists(experiment_path):\n",
        "  shutil.rmtree(experiment_path)\n",
        "\n",
        "scan_object = talos.Scan(\n",
        "    x=[fake_a, fake_b], y=fake_out,\n",
        "    params=hyperparams,\n",
        "    model=talos_model,\n",
        "    experiment_name=experiment_path,\n",
        "    time_limit=(datetime.datetime.now() +\n",
        "                datetime.timedelta(hours=40)).strftime(\"%Y-%m-%d %H:%M\"),\n",
        "    # reduction_method='correlation',\n",
        "    # reduction_interval=10,\n",
        "    # reduction_window=20,\n",
        "    # reduction_threshold=0.2,\n",
        "    # reduction_metric='NMSE',\n",
        "    minimize_loss=True,\n",
        "    save_weights=True,\n",
        "    print_params=True,\n",
        "    random_method='quantum'\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIlD1T9jzG_Z"
      },
      "source": [
        "\n",
        "patches_a, patches_b,distances = get_batch(1, height, p)\n",
        "sb=model.get_layer(\"single_branch\")\n",
        "# sb.predict([patches_a])\n",
        "for layer in sb.layers:\n",
        "  print(layer.name)\n",
        "\n",
        "sbf=keras.models.Model(inputs=sb.inputs, outputs=sb.layers[2].output)\n",
        "Xf=sbf.predict([patches_a])\n",
        "X=sb.predict([patches_a])\n",
        "print(X.shape)\n",
        "# plt.imshow(Xf[0,:,:,:]*10)\n",
        "plt.scatter(range(len(X[0,:])),X[0,:]);\n",
        "plt.show()\n",
        "# model.predict([patches_a, patches_b])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwNmlGHU7PHU"
      },
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "def save_object(obj, filename):\n",
        "    with open(filename, 'wb') as output:\n",
        "        pickle.dump(obj, output, protocol=2)\n",
        "\n",
        "\n",
        "def project_object(obj, *attributes):\n",
        "    out = {}\n",
        "    for a in attributes:\n",
        "        out[a] = getattr(obj, a)\n",
        "    return out\n",
        "\n",
        "\n",
        "pickle_path = os.path.join(\n",
        "    experiment_path, datetime.datetime.now().strftime(\"%Y%m%d_%H%M.pickle\"))\n",
        "t = project_object(scan_object, 'params', 'saved_models',\n",
        "                   'saved_weights', 'data', 'details', 'round_history')\n",
        "save_object(t, pickle_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2htQ5_QLk47k"
      },
      "source": [
        "\n",
        "Copy this on the console to prevent termination:\n",
        "```\n",
        "function ClickConnect() {\n",
        "console.log(\"Working\"); \n",
        "document\n",
        "  .querySelector('#top-toolbar > colab-connect-button')\n",
        "  .shadowRoot.querySelector('#connect')\n",
        "  .click() \n",
        "}\n",
        "setInterval(ClickConnect, 60000)\n",
        "```\n",
        "\n",
        "Interesting resources: \n",
        " - Hyperparameter optimization with talos\n",
        " - Tinyfying Neural Network with uTensor or TinyML\n",
        "\n"
      ]
    }
  ]
}