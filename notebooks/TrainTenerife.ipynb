{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainTenerife.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LOM9B7v7rgk"
      },
      "source": [
        "try:\n",
        "  #mount google drive\n",
        "  from google.colab import drive\n",
        "  drive_path=\"/content/drive\"\n",
        "  drive.mount(drive_path)\n",
        "  drive_folder=drive_path+\"/MyDrive/dtm/\"\n",
        "  using_colab=True\n",
        "except ModuleNotFoundError:\n",
        "  # Assume we are not on google colab, \n",
        "  drive_folder = \"data/\"\n",
        "  using_colab=False\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zCMo23j3v9b"
      },
      "source": [
        "# Params and Init\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32Lkw5cTBcE2"
      },
      "source": [
        "print(\"Importing    \")\n",
        "!pip -q install --upgrade pip\n",
        "# !pip -q install --upgrade tensorflow keras\n",
        "!pip -q install plyfile deepdiff talos GitPython numba\n",
        "!pip -q install --upgrade imgaug\n",
        "import talos\n",
        "from deepdiff import DeepDiff\n",
        "import re\n",
        "import numpy as np\n",
        "from progressbar import progressbar, ProgressBar\n",
        "import sys\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import json\n",
        "import importlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import git\n",
        "from tensorflow import keras\n",
        "import keras.backend as K\n",
        "import datetime\n",
        "import imgaug.augmenters as iaa\n",
        "cuda_paths=!ls /usr/local | grep cuda-\n",
        "os.environ['CUDA_HOME'] = os.path.join(\"/usr/local/\",cuda_paths[-1])\n",
        "K.manual_variable_initialization(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOra41HQBjZc"
      },
      "source": [
        "print(\"Importing deep_ga\")\n",
        "if using_colab:\n",
        "    !git -C /content/deep_ga/ pull || git clone https://github.com/InigoMoreno/deep_ga\n",
        "    sys.path.append('/content/deep_ga')\n",
        "    import deep_ga  # nopep8\n",
        "    deep_ga=importlib.reload(deep_ga)\n",
        "else:\n",
        "    MODULE_PATH = \"deep_ga/deep_ga/__init__.py\"\n",
        "    MODULE_NAME = \"deep_ga\"\n",
        "    git.cmd.Git(MODULE_NAME).pull()\n",
        "    spec = importlib.util.spec_from_file_location(MODULE_NAME, MODULE_PATH)\n",
        "    deep_ga = importlib.util.module_from_spec(spec)\n",
        "    sys.modules[spec.name] = deep_ga\n",
        "    spec.loader.exec_module(deep_ga)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xq_DnBKDEyL"
      },
      "source": [
        "p={\n",
        "  \"resolution\" : .5,                     # resolution of map [meters per pixel]\n",
        "  \"mapLength\"  : 40,                     # size of one side of the map [meters]\n",
        "  \"minSlopeThreshold\"  : 0.5,            # minimum slope to be counted [proportion]\n",
        "  \"maxNanPercentage\"   : 5/100,          # maximum percentage of NaNs in a patch [%]\n",
        "  \"minSlopePercentage\" : 0/100,          # minimum percentage of slope in a patch [%]\n",
        "  \"maxSlopePercentage\" : 100/100,        # maximum percentage of slope in a patch [%]\n",
        "  \"stdPatchShift\"          : 15,         # standard deviation of shift between to patches [m]\n",
        "  \"local_resolution\" : .5,\n",
        "  \"local_mapLength\"  : 40,\n",
        "  \"local_maxNanPercentage\": 65/100,\n",
        "  \"local_global_minCcorr\": 15/100,\n",
        "   \"booleanDist\": False\n",
        "   \n",
        "}\n",
        "p[\"mapLengthPixels\"]=math.ceil(p[\"mapLength\"]/p[\"resolution\"])\n",
        "p[\"local_mapLengthPixels\"]=math.ceil(p[\"local_mapLength\"]/p[\"local_resolution\"])\n",
        "deep_ga.set_scale(p[\"stdPatchShift\"])\n",
        "\n",
        "p[\"augment_a\"] = iaa.Sequential([\n",
        "    iaa.Add((-4, 4)),\n",
        "    iaa.PerspectiveTransform(scale=0.01),\n",
        "    iaa.BlendAlphaSimplexNoise(iaa.Add(0.2), upscale_method=\"cubic\", size_px_max=4),\n",
        "])\n",
        "\n",
        "\n",
        "hyperparams = {\n",
        "    # \"input\": [\"raw\", \"rawmask\", \"sobel\", \"fixsobel\", \"PConv\", \"SymConv\", \"Conv\"],\n",
        "    \"input\": [\"SymConv\"],\n",
        "    # \"batch_size\": [32, 64, 128, 256],\n",
        "    \"batch_size\": [4],\n",
        "    # \"mobileNet_alpha\": [1.0, 0.5, 0.35],\n",
        "    \"mobileNet_alpha\": [1.0],\n",
        "    # \"mobileNet_pooling\": [\"avg\", \"max\", None],\n",
        "    \"mobileNet_pooling\": [\"avg\"],\n",
        "    \"mobileNet_weights\": [None],\n",
        "    # \"mobileNet_weights\": [\"imagenet\"],\n",
        "    # \"firstLayerSize\": [4096, 1280, 1000],\"\n",
        "    \"firstLayerSize\": [0],\n",
        "    # \"dropout\": [0, 0.2, 0.5],\n",
        "    \"dropout\": [0.01],\n",
        "    # \"secondLayerSize\": [0, 100, 25],\n",
        "    \"secondLayerSize\": [50],\n",
        "    # \"activation\": [\"relu\", None],\n",
        "    \"activation\": [None],\n",
        "    # \"sharedWeights\": [True, False],\n",
        "    \"sharedWeights\": [True],\n",
        "    # \"loss\": [\"MSE\", \"MAE\", \"DOOMSE\", \"PCL\", \"BCE\"],\n",
        "    \"loss\": [\"DOOMSE\"],\n",
        "    \"optimizer\": [\"SGD\"],\n",
        "    # \"learning_rate\": [0.01, 0.001, 0.0001],\n",
        "    \"learning_rate\": [0.0001],\n",
        "    # \"learnEnding\": [True],\n",
        "    \"learnEnding\": [False],\n",
        "    # \"loadFolder\": [os.path.join(drive_folder,\n",
        "    #                             \"talos_oxia_mobileNet_alpha__secondLayerSize\")]\n",
        "}\n",
        "old_hyperparam_tests = [\n",
        "    {\"input\": [\"SymConv\", \"raw\", \"rawmask\", \"sobel\", \"fixsobel\", \"PConv\", \"Conv\"],\n",
        "    \"repeat\": [1,2,3]},#0\n",
        "    {\"displace\": [True, False],\n",
        "    \"repeat\": [1,2,3]}, #1\n",
        "    {\"batch_size\": [2, 4, 8, 16, 32]}, #2\n",
        "    {\"loss\": [\"MSE\", \"MAE\", \"DOOMSE\", \"PCL\"]}, #3\n",
        "    {\"optimizer\": [\"Adam\", \"SGD\"],\n",
        "     \"learning_rate\": [0.01, 0.001, 0.0001]}, #4\n",
        "    {\"mobileNet_alpha\": [1.4, 1.0, 0.75, 0.5, 0.35],\n",
        "    \"repeat\": [1,2,3]}, #5\n",
        "    {\"mobileNet_pooling\": [\"avg\", \"max\"],\n",
        "    \"repeat\": [1,2,3]}, #6\n",
        "    {\"mobileNet_weights\": [None,\"imagenet\"]}, #7\n",
        "    {\"firstLayerSize\": [500,100,50,25,20,15,10]}, #8\n",
        "    {\"dropout\": [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3],\n",
        "     \"secondLayerSize\": [100]}, #9\n",
        "    {\"activation\": [None, \"relu\"],\n",
        "     \"secondLayerSize\": [100],\n",
        "     \"repeat\": [1,2]}, #10\n",
        "    {\"sharedWeights\": [True, False],\n",
        "    \"repeat\": [1,2,3]}, #11\n",
        "]\n",
        "hyperparam_tests = [\n",
        "    {\"sharedWeights\": [True, False],\n",
        "    \"repeat\": [1,2,3]}, #0\n",
        "    {\"mobileNet_pooling\": [\"avg\", \"max\"],\n",
        "    \"repeat\": [1,2,3]}, #1\n",
        "    {\"input\": [\"SymConv\", \"raw\", \"rawmask\", \"sobel\", \"fixsobel\", \"PConv\", \"Conv\"],\n",
        "    \"arepeat\": [1,2,3]},#2\n",
        "    {\"displace\": [True, False],\n",
        "    \"arepeat\": [1,2,3]}, #3\n",
        "    {\"mobileNet_alpha\": [1.4, 1.0, 0.75, 0.5, 0.35],\n",
        "    \"arepeat\": [1,2,3]}, #4\n",
        "    {\"dropout\": [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3],\n",
        "     \"aarepeat\": [1,2]}, #5\n",
        "    {\"mobileNet_weights\": [None,\"imagenet\"],\n",
        "     \"arepeat\": [1,2,3]}, #6\n",
        "]\n",
        "\n",
        "\n",
        "if len(sys.argv)==2:\n",
        "    try: \n",
        "        idx=int(sys.argv[1])\n",
        "        hyperparams = {**hyperparams, **hyperparam_tests[idx]}\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "keys= [k for k,v in hyperparams.items() if len(v)>1]\n",
        "experiment_name=\"talos_tenerife_\"+\"__\".join(keys)\n",
        "experiment_path = os.path.relpath(os.path.join(drive_folder,experiment_name))\n",
        "print(experiment_name)\n",
        "first_hyperparams={k:v[0] for k,v in hyperparams.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chM5dlA1DJuk"
      },
      "source": [
        "# Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNdEb3EZDIaM"
      },
      "source": [
        "#get local_dems\n",
        "with np.load(os.path.join(drive_folder,\"processed\",\"filtered.npz\")) as data:\n",
        "  dems=data[\"dems\"]\n",
        "  gps= data[\"gps\"]\n",
        "#get global_dem\n",
        "ply_file=os.path.join(drive_folder,\"minas_densified_point_cloud.ply\")\n",
        "global_dem , global_img, displacement = deep_ga.ply_to_image(ply_file,p[\"resolution\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh3p8MjgU2Eq"
      },
      "source": [
        "day = np.vectorize(lambda a: datetime.datetime.utcfromtimestamp(a).day)(gps[:,0]/1e6)\n",
        "validation = day==8\n",
        "\n",
        "tdems=dems[~validation,:,:]\n",
        "vdems=dems[validation,:,:]\n",
        "tgps=gps[~validation,:]\n",
        "vgps=gps[validation,:]\n",
        "\n",
        "Nv=vdems.shape[0]\n",
        "Nt=tdems.shape[0]\n",
        "print(f\"Validation split: {Nv/(Nt+Nv)*100:.1f}%\")\n",
        "\n",
        "# del dems, gps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3imeAB-n16w"
      },
      "source": [
        "patches_a, patches_b,distances=deep_ga.get_batch_local_global(1,dems,gps,global_dem,displacement,p,seed=806)\n",
        "\n",
        "\n",
        "for i in range(1):\n",
        "  W=p[\"local_mapLength\"]/2\n",
        "  print(distances[i])\n",
        "  # ax=plt.subplot(1,2,1)\n",
        "\n",
        "  # plt.title(\"Local DEM\")\n",
        "  # plt.fig\n",
        "  # plt.xlabel(\"x [m]\")\n",
        "  # plt.ylabel(\"y [m]\")\n",
        "  plt.imshow(patches_a[i,:,:],extent=(-W,W,-W,W))\n",
        "  plt.savefig(f\"tenerife.pdf\",  dpi=500, bbox_inches = 'tight',\n",
        "    pad_inches = 0)\n",
        "  plt.show()\n",
        "\n",
        "  # W=p[\"mapLength\"]/2\n",
        "  # plt.subplot(1,2,2)\n",
        "  # plt.title(\"Global DEM\")\n",
        "  # plt.xlabel(\"x [m]\")\n",
        "  # # plt.ylabel(\"y [m]\")\n",
        "  plt.imshow(patches_b[i,:,:],extent=(-W,W,-W,W))\n",
        "  # # plt.savefig(f\"tenerife{seed}.pdf\",  dpi=500, bbox_inches = 'tight',\n",
        "  # #   pad_inches = 0)\n",
        "  plt.show()\n",
        "# plt.hist(distances)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayzRw-A3NogP"
      },
      "source": [
        "# Model preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcFUSViUybJ6"
      },
      "source": [
        "if using_colab:\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir {os.path.join(experiment_path,\"logs\")}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfcnmBMMUHTl"
      },
      "source": [
        "import datetime\n",
        "\n",
        "def talos_model(x_train, y_train, x_val, y_val, params):\n",
        "  global experiment_path, keys\n",
        "\n",
        "  values=[str(params[k]) for k in keys]\n",
        "  i=\"__\".join(values)\n",
        "  i=i.replace(\".\",\"_\")\n",
        "  print(i)\n",
        "\n",
        "\n",
        "  if \"displace\" in params.keys() and hyperparams[\"displace\"]:\n",
        "    dx,dy,dz = displacement\n",
        "    dx2=4\n",
        "    dy2=3\n",
        "    displace=(dx+dx2,dy+dy2,dz)\n",
        "  else:\n",
        "    displace=displacement\n",
        "\n",
        "  # get data generator\n",
        "  tgenerator=deep_ga.LocalGlobalPatchDataGenerator(tdems.shape[0]/5,params[\"batch_size\"],tdems,tgps,global_dem,displace,p)\n",
        "\n",
        "  # get validation data generator\n",
        "  vp=p.copy()\n",
        "  vp[\"augment_a\"]=None\n",
        "  vgenerator=deep_ga.LocalGlobalPatchDataGenerator(vdems.shape[0],params[\"batch_size\"],vdems,vgps,global_dem,displace,vp)\n",
        "  \n",
        "  with open(os.path.join(experiment_path, f\"{i}_params.json\"), 'w') as fp:\n",
        "      json.dump(params, fp)\n",
        "\n",
        "  callbacks=[]\n",
        "\n",
        "  # stop early if loss does not improve\n",
        "  callbacks.append(keras.callbacks.EarlyStopping(\n",
        "      monitor='val_loss', patience=20, verbose=1, restore_best_weights=True,min_delta=0.1))\n",
        "  \n",
        "  # assert that weights are finite\n",
        "  callbacks.append(deep_ga.AssertWeightsFinite())\n",
        "\n",
        "  # plot prediction\n",
        "  callbacks.append(deep_ga.PlotPrediction(\n",
        "      folder=os.path.join(experiment_path, f\"{i}_plots\"),\n",
        "      tgen=tgenerator,\n",
        "      vgen=vgenerator,\n",
        "      save=not using_colab\n",
        "    ))\n",
        "  \n",
        "    \n",
        "  # tensorboard callback\n",
        "  logdir = os.path.join(experiment_path,\"logs\", f\"{i}\")\n",
        "  callbacks.append(tf.keras.callbacks.TensorBoard(logdir))\n",
        "\n",
        "  # get model\n",
        "  input_a = keras.Input(shape=(\n",
        "      p[\"local_mapLengthPixels\"], p[\"local_mapLengthPixels\"]), name=\"patch_a\")\n",
        "  input_b = keras.Input(\n",
        "      shape=(p[\"mapLengthPixels\"], p[\"mapLengthPixels\"]), name=\"patch_b\")\n",
        "\n",
        "  model = deep_ga.get_model(params,input_a,input_b)\n",
        "  model = deep_ga.compile_model(model, y_val, params)\n",
        "\n",
        "  out = model.fit(tgenerator, \n",
        "                  validation_data=vgenerator,\n",
        "                  epochs=200, \n",
        "                  workers=6,\n",
        "                  callbacks=callbacks,\n",
        "                #   use_multiprocessing = True,\n",
        "                  verbose=(1 if using_colab else 2)\n",
        "                  )\n",
        "                  \n",
        "  x,y=tgenerator.get_batch(320)\n",
        "  yp=model.predict(x)[:,0]\n",
        "  print(f\"self mse loss = {keras.losses.MSE(y,yp)}\")\n",
        "  try:\n",
        "    with open(os.path.join(experiment_path, f\"{i}_structure.json\"), 'w') as fp:\n",
        "        fp.write(model.to_json())\n",
        "    model.save_weights(os.path.join(experiment_path, f\"{i}_weights.h5\"))\n",
        "    model.save(os.path.join(experiment_path, f\"{i}\"),include_optimizer=False)\n",
        "    model.save(os.path.join(experiment_path, f\"{i}.h5\"))\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  return out, model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWORyKGbxTr0"
      },
      "source": [
        "fake_local=np.empty((100,p[\"local_mapLengthPixels\"],p[\"local_mapLengthPixels\"]),np.float32)\n",
        "fake_global=np.empty((100,p[\"mapLengthPixels\"],p[\"mapLengthPixels\"]),np.float32)\n",
        "fake_out = np.empty(100)\n",
        "\n",
        "i=0\n",
        "import shutil\n",
        "if os.path.exists(experiment_path):\n",
        "  shutil.rmtree(experiment_path)\n",
        "\n",
        "scan_object = talos.Scan(\n",
        "    x=[fake_local, fake_global], y=fake_out, \n",
        "    params=hyperparams, \n",
        "    model=talos_model,\n",
        "    experiment_name = experiment_path,\n",
        "    time_limit = (datetime.datetime.now() + datetime.timedelta(hours=40)).strftime(\"%Y-%m-%d %H:%M\"),\n",
        "    minimize_loss=True,\n",
        "    save_weights=True,\n",
        "    print_params=True,\n",
        "    random_method='quantum',\n",
        "    val_split=0\n",
        "  )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuKT0_9oBPFc"
      },
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "def save_object(obj, filename):\n",
        "    with open(filename, 'wb') as output:\n",
        "        pickle.dump(obj, output, protocol=2)\n",
        "\n",
        "\n",
        "def project_object(obj, *attributes):\n",
        "    out = {}\n",
        "    for a in attributes:\n",
        "        out[a] = getattr(obj, a)\n",
        "    return out\n",
        "\n",
        "\n",
        "pickle_path = os.path.join(experiment_path, \"scan_object.pickle\")\n",
        "t = project_object(scan_object, 'params', 'saved_models',\n",
        "                   'saved_weights', 'data', 'details', 'round_history')\n",
        "save_object(t, pickle_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMS5PjXDqI02"
      },
      "source": [
        "\n",
        "Copy this on the console to prevent termination:\n",
        "```\n",
        "function ClickConnect() {\n",
        "console.log(\"Working\"); \n",
        "document\n",
        "  .querySelector('#top-toolbar > colab-connect-button')\n",
        "  .shadowRoot.querySelector('#connect')\n",
        "  .click() \n",
        "}\n",
        "setInterval(ClickConnect, 60000)\n",
        "```\n",
        "\n",
        "Interesting resources: \n",
        " - Hyperparameter optimization with talos\n",
        " - Tinyfying Neural Network with uTensor or TinyML\n",
        "\n"
      ]
    }
  ]
}